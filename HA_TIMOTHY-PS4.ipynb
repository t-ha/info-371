{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4, due May 16 at 11:59am, mid-day, noon.\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "This is a fun but challenging problem set. It will test your python skills, as well as your understanding of the material in class and in the readings. Start early and debug often! Some notes:\n",
    "\n",
    "* Part 1 is meant to be easy, so get through it quickly.\n",
    "* Part 2 (especially 2.1) will be difficult, but it is the lynchpin of this problem set to make sure to do it well and understand what you've done. If you find your gradient descent algorithm is taking more than a few minutes to complete, debug more, compare notes with others, and go to the TA sessions.\n",
    "* Depending on how well you've done 2.1, parts 2.3 and 3.3 will be relatively painless or incredibly painful. \n",
    "* Part 3 (especially 3.3) will be computationally intensive. Don't leave this until the last minute, otherwise your code might be running when the deadline arrives.\n",
    "* Do the extra credit problems last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction to the assignment\n",
    "\n",
    "As with the last assignment, you will be using the [Boston Housing Prices Data Set](http://archive.ics.uci.edu/ml/datasets/Housing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "bdata = load_boston()\n",
    "bdata.feature_names = np.append(bdata.feature_names, 'MEDV')\n",
    "boston = pd.DataFrame(bdata.data)\n",
    "boston['MEDV'] = bdata.target\n",
    "boston.columns = bdata.feature_names[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Getting oriented\n",
    "\n",
    "Use different learning rates\n",
    "\n",
    "### 1.1 Use existing libraries\n",
    "\n",
    "Soon, you will write your own gradient descent algorithm, which you will then use to minimize the squared error cost function.  First, however, let's use the canned versions that come with Python, to make sure we understand what we're aiming to achieve.\n",
    "\n",
    "Using the same Boston housing prices dataset, use the [Linear Regression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from sklearn or the [OLS class](http://wiki.scipy.org/Cookbook/OLS) from SciPy to explore the relationship between  median housing price and number of rooms per house. Do this by first (a) regressing the housing price on the number of rooms per house, and then (b) regressing the housing price on the number of rooms per house and the (number of rooms per house) squared.  **Interpret your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared:         </th> <td>   0.484</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.483</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   471.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 17 May 2016</td> <th>  Prob (F-statistic):</th> <td>2.49e-74</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>06:32:40</td>     <th>  Log-Likelihood:    </th> <td> -1673.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3350.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   504</td>      <th>  BIC:               </th> <td>   3359.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>  -34.6706</td> <td>    2.650</td> <td>  -13.084</td> <td> 0.000</td> <td>  -39.877   -29.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>        <td>    9.1021</td> <td>    0.419</td> <td>   21.722</td> <td> 0.000</td> <td>    8.279     9.925</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>102.585</td> <th>  Durbin-Watson:     </th> <td>   0.684</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 612.449</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.726</td>  <th>  Prob(JB):          </th> <td>1.02e-133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.190</td>  <th>  Cond. No.          </th> <td>    58.4</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   MEDV   R-squared:                       0.484\n",
       "Model:                            OLS   Adj. R-squared:                  0.483\n",
       "Method:                 Least Squares   F-statistic:                     471.8\n",
       "Date:                Tue, 17 May 2016   Prob (F-statistic):           2.49e-74\n",
       "Time:                        06:32:40   Log-Likelihood:                -1673.1\n",
       "No. Observations:                 506   AIC:                             3350.\n",
       "Df Residuals:                     504   BIC:                             3359.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -34.6706      2.650    -13.084      0.000       -39.877   -29.465\n",
       "RM             9.1021      0.419     21.722      0.000         8.279     9.925\n",
       "==============================================================================\n",
       "Omnibus:                      102.585   Durbin-Watson:                   0.684\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              612.449\n",
       "Skew:                           0.726   Prob(JB):                    1.02e-133\n",
       "Kurtosis:                       8.190   Cond. No.                         58.4\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one = smf.ols(formula='MEDV ~ RM', data=boston).fit()\n",
    "oneCoeff = round(one.params[1],2)\n",
    "oneInt = round(one.params[0],2)\n",
    "oneReg = one.predict()\n",
    "one.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared:         </th> <td>   0.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.547</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   305.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 17 May 2016</td> <th>  Prob (F-statistic):</th> <td>1.46e-87</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>06:32:40</td>     <th>  Log-Likelihood:    </th> <td> -1639.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3284.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   503</td>      <th>  BIC:               </th> <td>   3297.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>       <td>   66.0588</td> <td>   12.104</td> <td>    5.458</td> <td> 0.000</td> <td>   42.278    89.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>              <td>  -22.6433</td> <td>    3.754</td> <td>   -6.031</td> <td> 0.000</td> <td>  -30.019   -15.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(RM, 2)</th> <td>    2.4701</td> <td>    0.291</td> <td>    8.502</td> <td> 0.000</td> <td>    1.899     3.041</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>82.173</td> <th>  Durbin-Watson:     </th> <td>   0.689</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 934.337</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.224</td> <th>  Prob(JB):          </th> <td>1.29e-203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 9.642</td> <th>  Cond. No.          </th> <td>1.91e+03</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   MEDV   R-squared:                       0.548\n",
       "Model:                            OLS   Adj. R-squared:                  0.547\n",
       "Method:                 Least Squares   F-statistic:                     305.4\n",
       "Date:                Tue, 17 May 2016   Prob (F-statistic):           1.46e-87\n",
       "Time:                        06:32:40   Log-Likelihood:                -1639.1\n",
       "No. Observations:                 506   AIC:                             3284.\n",
       "Df Residuals:                     503   BIC:                             3297.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "-----------------------------------------------------------------------------------\n",
       "Intercept          66.0588     12.104      5.458      0.000        42.278    89.839\n",
       "RM                -22.6433      3.754     -6.031      0.000       -30.019   -15.267\n",
       "np.power(RM, 2)     2.4701      0.291      8.502      0.000         1.899     3.041\n",
       "==============================================================================\n",
       "Omnibus:                       82.173   Durbin-Watson:                   0.689\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              934.337\n",
       "Skew:                           0.224   Prob(JB):                    1.29e-203\n",
       "Kurtosis:                       9.642   Cond. No.                     1.91e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.91e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two = smf.ols(formula='MEDV ~ RM + np.power(RM, 2)', data=boston).fit()\n",
    "twoCoeff = round(two.params[1],2)\n",
    "twoCoeffS = round(two.params[2],2)\n",
    "twoInt = round(two.params[0],2)\n",
    "twoReg = two.predict()\n",
    "two.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBERSVATIONS\n",
    "\n",
    "If we compare the R-squared values we can see that adding number of rooms per house squared increases the value from 0.484 to 0.548. The increase in R-squared is expected because we are fitting the data with a variable we already used (RM). But, we need to be careful of overfitting the data when adding more features because it will not fit new data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training and testing\n",
    "\n",
    "Chances are, for the above problem you used all of your data to fit the regression line. In some circumstances this is a reasonable thing to do, but often this will result in overfitting. Let's redo the above results the ML way, using careful cross-validation.  Since you are now experts in cross-validation, and have written your own cross-validation algorithm from scratch, you can now take a shortcut and use the libraries that others have built for you.\n",
    "\n",
    "Using the [cross-validation functions](http://scikit-learn.org/stable/modules/cross_validation.html) from scikit-learn, use k-fold cross-validation to fit regression (a) above, i.e. the linear fit of housing price on number of rooms per house. Each fold of cross-validation will give you one slope coefficient and one intercept coefficient.  Plot the distribution of the k slope coefficients, and draw a vertical line at the value of the slope coefficient that you estimated in 1.1 using the full dataset.  What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coeffs = []\n",
    "numF = 100\n",
    "kf = sklearn.cross_validation.KFold(len(boston), n_folds=numF)\n",
    "for train, test in kf:\n",
    "    trainSet = boston.ix[train]\n",
    "    testSet = boston.ix[test]\n",
    "    regr = smf.ols(formula='MEDV ~ RM', data=trainSet).fit()\n",
    "    coeffs.append(regr.params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAHuCAYAAADp3rSVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEQAAAFBB/x+pOnRDp0+SNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a86ccd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "x = coeffs\n",
    "y = range(1, numF + 1)\n",
    "ax.scatter(x, y, color='blue')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "# ax.set_xlim(xmin=-1)\n",
    "ax.set_xlim([8,10])\n",
    "ax.axvline(x=oneCoeff, linewidth=2, color='black')\n",
    "\n",
    "plt.ylabel('k iteration')\n",
    "plt.xlabel('RM slope coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATION\n",
    "\n",
    "The slope coefficients are pretty close to the vertical line with some variation of course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Regression lines\n",
    "\n",
    "Create a scatterplot of housing price on rooms per house, and add the two regression lines from 1.1 (or 1.2 if you prefer to do so). Show the linear regression line in red, and the linear+quadratic regression line (which may have curvature) in blue.  Label these two regression lines with the corresponding regression equations (i.e. the slope and intercept of the line).\n",
    "\n",
    "Bonus: Add the 95% confidence bands (i.e.,  the area that has a 95% chance of containing the true regression line) to each of these lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAHuCAYAAACYkKX6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUkqlOA32SimlVIrTYK+UUkqluP8HGWqvZN75iQ8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a88e510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "x = boston['RM']\n",
    "y = boston['MEDV']\n",
    "ax.scatter(x, y, color='green')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.set_xlim(xmin=-1)\n",
    "oneRegLine = ax.plot(x, oneReg, color='red')\n",
    "twoRegLine = ax.plot(x, twoReg, color='blue')\n",
    "\n",
    "plt.ylabel('Median Housing Value')\n",
    "plt.xlabel('# rooms per house')\n",
    "plt.legend([str(oneCoeff) + 'x + ' + str(oneInt), str(twoCoeffS) + 'x2 + ' + str(twoCoeff) + 'x + ' + str(twoInt)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradient descent: Linear Regression\n",
    "\n",
    "This is where it gets fun!\n",
    "\n",
    "### 2.1 Implement gradient descent with one independent variable (Average rooms per house)\n",
    "\n",
    "Implement the basic gradient descent algorithm that we discussed in class. Use the version you implement to regress the housing price on the number of rooms per house. Experiment with 3-4 different values of the learning rate *R*, and do the following:\n",
    "\n",
    "* Report the values of alpha and beta that minimize the loss function\n",
    "* Report the number of iterations it takes for your algorithm to converge (for each value of *R*)\n",
    "* Report the total running time of your algorithm, in seconds\n",
    "\n",
    "Some skeleton code is provided below, but you should feel free to delete this code and start from scratch if you prefer.\n",
    "\n",
    "* *Hint 1: Don't forget to implement a stopping condition, so that at every iteration you check whether your results have converged. Common approaches to this are to (a) check to see if the loss has stopped decreasing; and (b) check if both your current parameter esimates are close to the estimates from the previous iteration.  In both cases, \"close\" should not be ==0, it should be <=epsilon, where epsilon is something very small (like 0.0001).*\n",
    "* *Hint 2: Some people like to include a MaxIterations parameter in their gradient descent algorithm, to prevent divergence. *\n",
    "* *Hint 3: R = 0.005 is a reasonable first guess - but try some others to see how it affects your results. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "bivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalues, yvalues : narray\n",
    "    xvalues: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta: float\n",
    "    co-efficient\n",
    "numIter: int\n",
    "    number of iterations ran\n",
    "\"\"\"\n",
    "def bivariate_ols(xvalues, yvalues, R, MaxIterations, e):\n",
    "    start_time = time.time()\n",
    "    alpha = 0\n",
    "    beta = 0\n",
    "    N = len(xvalues)\n",
    "    numIter = 0\n",
    "    close = False\n",
    "    costPrev = (1.0 / (2 * N)) * (np.sum(alpha + beta * xvalues - yvalues) ** 2)\n",
    "    while MaxIterations > 0 and not close:\n",
    "        alpha = alpha - (R / N) * np.sum(alpha + beta * xvalues - yvalues)\n",
    "        beta = beta - (R / N) * np.sum((alpha + beta * xvalues - yvalues) * xvalues)\n",
    "        cost = (1.0 / (2 * N)) * (np.sum(alpha + beta * xvalues - yvalues) ** 2)\n",
    "        close = abs(cost - costPrev) <= e\n",
    "        costPrev = cost\n",
    "        numIter += 1\n",
    "        MaxIterations -= 1\n",
    "\n",
    "    print \"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\"\n",
    "    return alpha, beta, numIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(boston['RM'])\n",
    "Y = np.array(boston['MEDV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.73 seconds\n",
      "(-33.619927856099181, 8.9369542970990032, 28931)\n"
     ]
    }
   ],
   "source": [
    "# R = 0.01\n",
    "print bivariate_ols(X, Y, 0.01, 100000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1.29 seconds\n",
      "(-33.173706881458131, 8.8668004008399421, 52285)\n"
     ]
    }
   ],
   "source": [
    "# R = 0.005\n",
    "print bivariate_ols(X, Y, 0.005, 100000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.01 seconds\n",
      "(0.43759643303959861, 3.5829650199082246, 291)\n"
     ]
    }
   ],
   "source": [
    "# R = 0.001\n",
    "print bivariate_ols(X, Y, 0.001, 100000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.21 seconds\n",
      "(-34.228073037743847, 9.0325811169246251, 6929)\n"
     ]
    }
   ],
   "source": [
    "# R = 0.05\n",
    "print bivariate_ols(X, Y, 0.05, 100000, 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATION\n",
    "\n",
    "We se that using a R of 0.05 we get the most accurate slope and coefficient values compared to 1.1 with some of the quickest time. Interestingly, using very small values of R yields very innacurate values and a low number of iterations. This is most likely due to the way I wrote the code and because R is so small, the function we're trying to minimize changes by very little so we get a difference of less than epsilon quicker than we get accurate slope/coefficient values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data normalization\n",
    "\n",
    "Soon, you will implement a version of gradient descent that can use an arbitrary number of independent variables. Before doing this, however, you should re-scale your features to ensure that no single feature dominates the cost function. Write a simple function to [standardize](http://en.wikipedia.org/wiki/Standard_score) a feature. * This is done for you!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def standardize(raw_data):\n",
    "    return ((raw_data - np.mean(raw_data, axis = 0)) / np.std(raw_data, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Implement gradient descent with an arbitrary number of independent variables\n",
    "\n",
    "Now that you have a simple version of gradient descent working, create a version of gradient descent that can take more than one independent variable.  Assume all independent variables will be continuous.  Test your algorithm using CRIM and RM as independent variables. Standardize these variables before before inputting them to the gradient descent algorithm. \n",
    "\n",
    "As before,  report and interpret your estimated coefficients, the number of iterations before convergence, and the total running time of your algorithm. Experiment with 2-3 different values of R.\n",
    "\n",
    "* *Hint 1: Be careful to implement this efficiently, otherwise it might take a long time for your code to run. Commands like `np.dot` can be a good friend to you on this problem - see 2.2 above for an example.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "multivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalues, yvalues : narray\n",
    "    xvalues: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta_array: array[float]\n",
    "    co-efficient\n",
    "\"\"\"\n",
    "\n",
    "def multivariate_ols(xvalues, yvalues, R, MaxIterations, e):\n",
    "    start_time = time.time()\n",
    "    alpha = 0\n",
    "    beta_array = np.zeros(xvalues.shape[1])\n",
    "    N = xvalues.shape[0]\n",
    "    numIter = 0\n",
    "    close = False\n",
    "    costPrev = (1.0 / (2 * N)) * (np.sum(alpha + np.dot(beta_array, xvalues.T) - yvalues) ** 2)\n",
    "    while MaxIterations > 0 and not close:\n",
    "        alpha = alpha - (R / N) * np.sum(alpha + np.dot(beta_array, xvalues.T) - yvalues)\n",
    "        beta_array = beta_array - (R / N) * np.sum(np.dot(alpha + np.dot(beta_array, xvalues.T) - yvalues, xvalues))\n",
    "        cost = (1.0 / (2 * N)) * (np.sum(alpha + np.dot(beta_array, xvalues.T) - yvalues) ** 2)\n",
    "        close = abs(cost - costPrev) <= e\n",
    "        costPrev = cost\n",
    "        numIter += 1\n",
    "        MaxIterations -= 1\n",
    "    print \"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\"\n",
    "    return alpha, beta_array, numIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XX = np.array(boston[['RM', 'CRIM']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0 seconds\n",
      "(22.532413646386306, array([ 1.82290361,  1.82290361]), 104)\n"
     ]
    }
   ],
   "source": [
    "print multivariate_ols(standardize(XX), Y, 0.1, 100000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.05 seconds\n",
      "(22.531423464569595, array([ 1.82290318,  1.82290318]), 965)\n"
     ]
    }
   ],
   "source": [
    "print multivariate_ols(standardize(XX), Y, 0.01, 100000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.3 seconds\n",
      "(22.528367183771962, array([ 1.82290064,  1.82290064]), 8528)\n"
     ]
    }
   ],
   "source": [
    "print multivariate_ols(standardize(XX), Y, 0.001, 100000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 2.26 seconds\n",
      "(22.518750194167655, array([ 1.82288543,  1.82288543]), 73793)\n"
     ]
    }
   ],
   "source": [
    "print multivariate_ols(standardize(XX), Y, 0.0001, 100000, 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATIONS\n",
    "\n",
    "All runs of the standardized x values (RM, CRIM) with varying learning rates, R, yeilded very similar alpha and beta array values. Larger learning rate values were much quicker due to less iterations and smaller R values took longer due to more iterations. I can't speak for what the impacts of standardization were because I have not yet looked at it with unstandardized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compare standardized vs. non-standardized results\n",
    "\n",
    "Repeat the analysis from 2.3, but this time do not standardize your variables - i.e., use the original data. Use the same three values of R (0.1, 0.01, and 0.001). What do you notice about the running time and convergence properties of your algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R=0.1\n",
      "Time taken: 4.42 seconds\n",
      "(nan, array([ nan,  nan]), 100000)\n",
      "\n",
      "R=0.01\n",
      "Time taken: 0.08 seconds\n",
      "(26.109522985884578, array([-0.36229228, -0.36229228]), 2010)\n",
      "\n",
      "R=0.001\n",
      "Time taken: 0.58 seconds\n",
      "(26.098382030904848, array([-0.36163929, -0.36163929]), 17489)\n",
      "\n",
      "R=0.0001\n",
      "Time taken: 3.46 seconds\n",
      "(25.73097998789202, array([-0.34013396, -0.34013396]), 100000)\n"
     ]
    }
   ],
   "source": [
    "print 'R=0.1'\n",
    "print multivariate_ols(XX, Y, 0.1, 100000, 0.00001)\n",
    "print\n",
    "print 'R=0.01'\n",
    "print multivariate_ols(XX, Y, 0.01, 100000, 0.00001)\n",
    "print\n",
    "print 'R=0.001'\n",
    "print multivariate_ols(XX, Y, 0.001, 100000, 0.00001)\n",
    "print\n",
    "print 'R=0.0001'\n",
    "print multivariate_ols(XX, Y, 0.0001, 100000, 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATIONS\n",
    "\n",
    "Right away we see for R=0.1 that the algorithim probably overshot the minimum and never converges even after 100,000 iterations. The runtime of R=0.01, 0.001 are roughly twice as long as it is using the standardized data. We see a difference in convergence for the alpha values which were around 22.53 but are now 26.1 and different beta values as well. We expect difference values of convergence because 'CRIM' values have a much higher range than 'RM' and it would not be accurately represented within the cost function as 'CRIM' would have more weight than 'RM'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction\n",
    "\n",
    "Let's use our fitted model to make predictions about housing prices. Since the focus is now on prediction rather than the interpretation of the coefficients, make sure to use the standardized version of your features in everything that follows.\n",
    "\n",
    "### 3.1 Cross-Validation\n",
    "\n",
    "Unless you were careful above, you probably overfit your data again.  Use 10-fold cross-validation to re-fit the multivariate regression from 2.3 above, and report your estimated coefficients (there should be three coefficients for each fold, corresponding to the intercept and the two coefficients for CRIM and RM). \n",
    "\n",
    "How do your estimated coefficients from cross-validation compare to the ones you estimated in 2.3 above? How do they compare to the ones estimated using standard packages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.28 seconds\n",
      "Time taken: 0.26 seconds\n",
      "Time taken: 0.3 seconds\n",
      "Time taken: 0.28 seconds\n",
      "Time taken: 0.29 seconds\n",
      "Time taken: 0.27 seconds\n",
      "Time taken: 0.27 seconds\n",
      "Time taken: 0.28 seconds\n",
      "Time taken: 0.27 seconds\n",
      "Time taken: 0.28 seconds\n",
      "[[ 22.75751634   1.76883939   1.76883939]\n",
      " [ 22.3282857    1.81923496   1.81923496]\n",
      " [ 23.02674826   1.76482941   1.76482941]\n",
      " [ 21.55312231   1.40124746   1.40124746]\n",
      " [ 21.94499041   1.58476742   1.58476742]\n",
      " [ 21.53356143   1.36260822   1.36260822]\n",
      " [ 22.60321701   1.83476313   1.83476313]\n",
      " [ 23.08676893   2.46486851   2.46486851]\n",
      " [ 23.50628649   2.34189875   2.34189875]\n",
      " [ 22.93633283   1.85707023   1.85707023]]\n"
     ]
    }
   ],
   "source": [
    "numF = 10\n",
    "coeffs31 = np.empty([numF, 3])\n",
    "kf = sklearn.cross_validation.KFold(len(Y), n_folds=numF)\n",
    "count = 0\n",
    "for train, test in kf:\n",
    "    trainSet = boston.ix[train]\n",
    "    Y31 = np.array(trainSet['MEDV'])\n",
    "    XX31 = standardize(np.array(trainSet[['RM', 'CRIM']]))\n",
    "    a, b, i = multivariate_ols(XX31, Y31, 0.001, 100000, 0.00001)\n",
    "    coeffs31[count][0] = a\n",
    "    coeffs31[count][1] = b[0]\n",
    "    coeffs31[count][2] = b[1]\n",
    "    count += 1\n",
    "print coeffs31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATIONS\n",
    "\n",
    "In general, the alpha values and the values of the beta arrays after 10-fold CV are comparable to the values we got in 2.3. There is of course some variation in the data with high alphas of 23.5 and lows of 21.5 and high betas of 2.46 and low betas of 1.4 but this is expected because we're only using 90% of the data as training data per fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Predicted values and RMSE\n",
    "\n",
    "Let's figure out how accurate this predictive model turned out to be. Report the average 10-fold cross-validated RMSE, separately for the training data and for the testing data. \n",
    "\n",
    "In other words, run 10-fold cross-validation. In each of the 10 iterations, you will fit a model on 90% of the data. Use that model to generate predicted outputs for 100% of the data. For that iteration, the training RMSE is the RMSE calculated across the (90%) training data, and the test RMSE is the RMSE calculated across the (10%) test data. The average 10-fold cross-validated RMSE is the average of the 10 iterations.\n",
    "\n",
    "What is your test RMSE?  How does it compare to the performance of your nearest neighbor algorithm from the last problem set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_rmse(predictions, yvalues):\n",
    "    rmse = 0.0\n",
    "    for pre, yval in zip(predictions, yvalues):\n",
    "        rmse += (pre - yval) ** 2\n",
    "    rmse /= len(predictions)\n",
    "    return (rmse ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.28 seconds\n",
      "Time taken: 0.19 seconds\n",
      "Time taken: 0.29 seconds\n",
      "Time taken: 0.21 seconds\n",
      "Time taken: 0.29 seconds\n",
      "Time taken: 0.19 seconds\n",
      "Time taken: 0.3 seconds\n",
      "Time taken: 0.22 seconds\n",
      "Time taken: 0.3 seconds\n",
      "Time taken: 0.21 seconds\n",
      "Time taken: 0.5 seconds\n",
      "Time taken: 0.49 seconds\n",
      "Time taken: 0.42 seconds\n",
      "Time taken: 0.51 seconds\n",
      "Time taken: 0.74 seconds\n",
      "Time taken: 0.41 seconds\n",
      "Time taken: 0.34 seconds\n",
      "Time taken: 0.27 seconds\n",
      "Time taken: 0.27 seconds\n",
      "Time taken: 0.19 seconds\n",
      "RMSE: 4.15123978092\n"
     ]
    }
   ],
   "source": [
    "numF = 10\n",
    "cumRMSE = []\n",
    "kf = sklearn.cross_validation.KFold(len(Y), n_folds=numF)\n",
    "count = 0\n",
    "for train, test in kf:\n",
    "    trainSet32 = boston.ix[train]\n",
    "    testSet32 = boston.ix[test]\n",
    "    Y32 = np.array(trainSet32['MEDV'])\n",
    "    XX32 = standardize(np.array(trainSet32[['RM', 'CRIM']]))\n",
    "    a, b, i = multivariate_ols(XX32, Y32, 0.001, 100000, 0.00001)\n",
    "    Y32t = np.array(testSet32['MEDV'])\n",
    "    XX32t = standardize(np.array(testSet32[['RM', 'CRIM']]))\n",
    "    at, bt, it = multivariate_ols(XX32t, Y32t, 0.001, 100000, 0.00001)\n",
    "    cumRMSE.append(compute_rmse([at, bt], [a, b]))\n",
    "    count += 1\n",
    "print 'RMSE: ' + str(np.mean(cumRMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATIONS\n",
    "\n",
    "The RMSE value was 4.15. This is much lower than what I had for my KNN algorithim but I'm pretty sure the algorithm wasn't 100% correct. Regardless, the RMSE is a lot lower than I had previously expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit: Logistic Regression\n",
    "\n",
    "For extra credit, implement logistic regression using gradient descent. Create a new variable (EXPENSIVE) to indicate whether the median housing price is more than $400,000. Use your model  a logistic regression of EXPENSIVE on CHAS and RM. Report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Regularization \n",
    "\n",
    "### 4.1 Get prepped\n",
    "\n",
    "Step 1: For each of the continuous features F in the original dataset, create a standardized version F_1.  Now, create polynomials up to degree 6 of each F_1: the square of F_1 (call this F_2); the cube of F_1 (call this F_3); and so forth up to F_6. If you originally had *K* features, you should now have *6K* features (i.e., we're going to ignore the original unscaled features for the remainder of this problem).\n",
    "\n",
    "Step 2: For simplicity, generate a single training and testing set.  Randomly sample 66% of your data and call this the training set, and set aside the remaining 34% as your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM  INDUS    NOX     RM   AGE     DIS  TAX  PTRATIO       B  LSTAT\n",
       "0  0.00632   2.31  0.538  6.575  65.2  4.0900  296     15.3  396.90   4.98\n",
       "1  0.02731   7.07  0.469  6.421  78.9  4.9671  242     17.8  396.90   9.14\n",
       "2  0.02729   7.07  0.469  7.185  61.1  4.9671  242     17.8  392.83   4.03\n",
       "3  0.03237   2.18  0.458  6.998  45.8  6.0622  222     18.7  394.63   2.94\n",
       "4  0.06905   2.18  0.458  7.147  54.2  6.0622  222     18.7  396.90   5.33"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newBoston = boston\n",
    "newBoston = newBoston.drop(['CHAS', 'RAD', 'ZN', 'MEDV'], axis=1)\n",
    "newBoston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for name in newBoston.columns.values:\n",
    "    for i in range(1, 7):\n",
    "        df[name + '_' + str(i)] = np.power(newBoston[name], i)\n",
    "df['MEDV'] = boston['MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Let's overfit!\n",
    "Now, using your version of multivariate regression from 2.3, (over)fit your model on the training data. Using your training set, regress housing price on as many of those *6K* features as you can.  If you get too greedy, or if you did not efficiently implement your solution to 2.3, it's possible this will take a long time to compute.\n",
    "\n",
    "Report the RMSE when you apply your model to your training set and to your testing set. How do these numbers compare to each other, and to the RMSE from 2.5 and nearest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM_1', 'CRIM_2', 'CRIM_3', 'CRIM_4', 'CRIM_5', 'CRIM_6',\n",
       "       'INDUS_1', 'INDUS_2', 'INDUS_3', 'INDUS_4', 'INDUS_5', 'INDUS_6',\n",
       "       'NOX_1', 'NOX_2', 'NOX_3', 'NOX_4', 'NOX_5', 'NOX_6', 'RM_1',\n",
       "       'RM_2', 'RM_3', 'RM_4', 'RM_5', 'RM_6', 'AGE_1', 'AGE_2', 'AGE_3',\n",
       "       'AGE_4', 'AGE_5', 'AGE_6', 'DIS_1', 'DIS_2', 'DIS_3', 'DIS_4',\n",
       "       'DIS_5', 'DIS_6', 'TAX_1', 'TAX_2', 'TAX_3', 'TAX_4', 'TAX_5',\n",
       "       'TAX_6', 'PTRATIO_1', 'PTRATIO_2', 'PTRATIO_3', 'PTRATIO_4',\n",
       "       'PTRATIO_5', 'PTRATIO_6', 'B_1', 'B_2', 'B_3', 'B_4', 'B_5', 'B_6',\n",
       "       'LSTAT_1', 'LSTAT_2', 'LSTAT_3', 'LSTAT_4', 'LSTAT_5', 'LSTAT_6',\n",
       "       'MEDV'], dtype=object)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.45 seconds\n",
      "Time taken: 0.35 seconds\n",
      "RMSE: 0.233801210897\n"
     ]
    }
   ],
   "source": [
    "Y42 = np.array(df_train['MEDV'])\n",
    "XX42 = standardize(np.array(df_train.drop(['MEDV'], axis=1)))\n",
    "a, b, i = multivariate_ols(XX42, Y42, 0.001, 100000, 0.00001)\n",
    "\n",
    "Y42t = np.array(df_test['MEDV'])\n",
    "XX42t = standardize(np.array(df_test.drop(['MEDV'], axis=1)))\n",
    "at, bt, it = multivariate_ols(XX42t, Y42t, 0.001, 100000, 0.00001)\n",
    "print 'RMSE: ' + str(compute_rmse([a, b[0], b[1]], [at, bt[0], bt[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATIONS\n",
    "\n",
    "I'm not sure if we're supposed to standardize the feature data or not. I feel like we weren't supposed to but I couldn't get beyond 2 degrees without all my alpha/beta values being NaN no matter what R, MaxIteration, or e values I passed in to the multivariate_ols function. That being said, using standardized features yielded a RMSE of 0.233. I feel as though the RMSE SHOULD be low because we are purposely overfitting the data, meaning there should be a much lower amount of error between the training and test data. Obviously overfitting data is bad because it does not generalize well to more data, but I'm thinking(hoping) that that is the point of 4.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Ridge regularization\n",
    "Incorporate L2 (Ridge) regularization into your multivariate_ols regression. Write a new version of your gradient descent algorithm that includes a regularization term \"lambda\" to penalize excessive complexity. \n",
    "\n",
    "Use your regularized regression to re-fit the model from 4.2 above on your training data. Try this for several different values of lambda, and report your RMSE for each lambda separately for your training and testing data. How do these numbers compare to each other, to the RMSE from 4.2,  to the RMSE from 2.3, and to the RMSE from nearest neighbors?\n",
    "\n",
    "Go brag to your friends about how you just implemented ridge-regularized multivariate regression using gradient descent optimization, from scratch. If you still have friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ridge_reg(xvalues, yvalues, R, MaxIterations, e, l):\n",
    "    start_time = time.time()\n",
    "    alpha = 0\n",
    "    beta_array = np.zeros(xvalues.shape[1])\n",
    "    N = xvalues.shape[0]\n",
    "    numIter = 0\n",
    "    close = False\n",
    "    costPrev = (1.0 / (2 * N)) * (np.sum(alpha + np.dot(beta_array, xvalues.T) - yvalues) ** 2) + l * np.sum(beta_array**2)\n",
    "    while MaxIterations > 0 and not close:\n",
    "        alpha = alpha - (R / N) * np.sum(alpha + np.dot(beta_array, xvalues.T) - yvalues)\n",
    "        beta_array = beta_array * (1.0 - R * l / N) - (R / N) * np.sum(np.dot(alpha + np.dot(beta_array, xvalues.T) - yvalues, xvalues))\n",
    "        cost = (1.0 / (2 * N)) * (np.sum(alpha + np.dot(beta_array, xvalues.T) - yvalues) ** 2) + l * np.sum(beta_array**2)\n",
    "        close = abs(cost - costPrev) <= e\n",
    "        costPrev = cost\n",
    "        numIter += 1\n",
    "        MaxIterations -= 1\n",
    "    print \"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\"\n",
    "    return alpha, beta_array, numIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.5 seconds\n",
      "Time taken: 0.43 seconds\n",
      "RMSE: 0.13869012182\n"
     ]
    }
   ],
   "source": [
    "Y43 = np.array(df_train['MEDV'])\n",
    "XX43 = standardize(np.array(df_train.drop(['MEDV'], axis=1)))\n",
    "a, b, i = ridge_reg(XX43, Y43, 0.001, 100000, 0.00001, 10000)\n",
    "\n",
    "Y43t = np.array(df_test['MEDV'])\n",
    "XX43t = standardize(np.array(df_test.drop(['MEDV'], axis=1)))\n",
    "at, bt, it = ridge_reg(XX43t, Y43t, 0.001, 100000, 0.00001, 10000)\n",
    "print 'RMSE: ' + str(compute_rmse([a, b[0], b[1]], [at, bt[0], bt[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATIONS\n",
    "\n",
    "Using a high lambda value yields a higher RMSE. I'm not exactly sure what the \"range\" of lambda can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit 2: Cross-validate lambda\n",
    "\n",
    "Use k-fold cross-validation to select the optimal value of lambda. Report the average RMSE across all training sets, and the average RMSE across all testing sets. How do these numbers compare to each other, to the RMSE from your previous efforts?  Finally, create a scatter plot that shows RMSE as a function of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Showoff) Extra Credit 3: Lambda and coefficients\n",
    "\n",
    "If you're feeling extra-special, create a parameter plot that shows how the different coefficient estimates change as a function of lambda. To make this graph intelligible, only include the *K* original F_s features in this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
